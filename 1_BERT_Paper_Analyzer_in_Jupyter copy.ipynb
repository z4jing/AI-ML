{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "# # 医疗文献相关性分析工具 (基于开源 Chinese-BERT-WWM)\n",
        "#\n",
        "# **目标:** 基于用户输入的疾病名称，利用本地加载的 Chinese-BERT WWM 模型对论文摘要和标题进行零样本分类，给出相关性评级（高、中、低）。\n",
        "#\n",
        "# **运行环境:** 建议在 Jupyter Notebook 或 VS Code Notebook 中运行。\n",
        "#\n",
        "# **❗重要提示:** # 1. 本方法使用 Hugging Face 的 Zero-Shot Classification Pipeline。\n",
        "# 2. 首次运行需下载模型，需耐心等待。\n",
        "# 3. 推荐使用 GPU 运行以获得可接受的速度。\n",
        "#\n",
        "# **依赖库安装:**\n",
        "# 请在运行前确保安装了以下库：\n",
        "# ```bash\n",
        "# # 安装 PyTorch (根据您的系统可能不同，请参考 PyTorch 官网)\n",
        "# # pip install torch\n",
        "#\n",
        "# pip install pandas tqdm transformers\n",
        "# ```\n",
        "\n",
        "# %%\n",
        "import json\n",
        "import pandas as pd\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import random\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# 导入 Hugging Face 库\n",
        "from transformers import pipeline\n",
        "\n",
        "# --- 模型配置 ---\n",
        "# 推荐使用 Zero-Shot Classification 管道\n",
        "CLASSIFIER = None\n",
        "# 已修正模型名称。原 'uer/chinese-roberta-wwm-ext-large' 不存在，改为 HFL 维护的标准大规模中文 RoBERTa 模型\n",
        "MODEL_NAME = \"hfl/chinese-roberta-wwm-ext-large\"\n",
        "LABELS = [\"高\", \"中\", \"低\"] # 预设的分类标签\n",
        "# 用于零样本分类的假设模板（可以根据疾病调整）\n",
        "# 例如: \"这篇文章是关于 [疾病名称] 的。\"\n",
        "HYPOTHESIS_TEMPLATE = \"这篇文章与疾病 {disease} 的关联度是{}\"\n",
        "\n",
        "# --- 全局配置 ---\n",
        "FILE_PATH = 'papers.json'\n",
        "OUTPUT_FILE_NAME = 'analyzed_papers_bert.csv'\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 1. 初始化模型\n",
        "#\n",
        "# 加载 Hugging Face 的 Zero-Shot Classification Pipeline。\n",
        "\n",
        "# %%\n",
        "def initialize_classifier():\n",
        "    \"\"\"初始化并加载本地模型和 Pipeline。\"\"\"\n",
        "    global CLASSIFIER\n",
        "    print(f\"--- 1. 正在加载模型: {MODEL_NAME} ---\")\n",
        "    try:\n",
        "        # 使用 zero-shot-classification 管道\n",
        "        # 首次运行时会自动下载模型\n",
        "        CLASSIFIER = pipeline(\n",
        "            \"zero-shot-classification\",\n",
        "            model=MODEL_NAME,\n",
        "            device=-1 # -1 for CPU, 0 or greater for GPU index\n",
        "        )\n",
        "        print(\"模型加载成功，设备设置为 CPU/GPU。\")\n",
        "    except Exception as e:\n",
        "        print(f\"致命错误: 模型加载失败。请检查 'transformers' 和 'torch' 是否正确安装。错误: {e}\")\n",
        "        CLASSIFIER = None\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 2. BERT 本地分类函数\n",
        "#\n",
        "# 定义函数，使用加载的模型进行相关性分类。\n",
        "\n",
        "# %%\n",
        "def classify_relevance_with_bert(paper_title: str, paper_abstract: str, disease_name: str) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    使用 Hugging Face 的零样本分类 Pipeline 评估论文与疾病的相关性。\n",
        "    \"\"\"\n",
        "    if not CLASSIFIER:\n",
        "        return {\"relevance\": \"N/A\", \"reason\": \"模型未初始化\"}\n",
        "\n",
        "    # 将标题和摘要拼接起来作为输入文本\n",
        "    text_to_classify = f\"标题: {paper_title}。摘要: {paper_abstract}\"\n",
        "\n",
        "    # 零样本分类调用\n",
        "    try:\n",
        "        # 分类结果包含每个标签的得分\n",
        "        result = CLASSIFIER(\n",
        "            text_to_classify,\n",
        "            LABELS,\n",
        "            hypothesis_template=HYPOTHESIS_TEMPLATE.format(disease=disease_name),\n",
        "            multi_label=False # 确保只选择一个最高标签\n",
        "        )\n",
        "\n",
        "        # 提取最高得分的标签作为相关性结果\n",
        "        relevance = result['labels'][0]\n",
        "        score = result['scores'][0]\n",
        "\n",
        "        # 构建理由\n",
        "        reason = f\"基于零样本分类，模型认为该文本与 '{relevance}' 的相似度最高 (置信度: {score:.2f})。\"\n",
        "\n",
        "        return {\"relevance\": relevance, \"reason\": reason}\n",
        "\n",
        "    except Exception as e:\n",
        "        # 模型推理失败（可能是输入过长等原因）\n",
        "        return {\"relevance\": \"N/A\", \"reason\": f\"BERT 推理失败: {e}\"}\n",
        "\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 3. 数据加载与分析主流程\n",
        "#\n",
        "# 定义主函数来处理数据并调用本地分类模型。\n",
        "\n",
        "# %%\n",
        "def analyze_papers_bert(file_path: str):\n",
        "    \"\"\"\n",
        "    主函数：加载数据、获取用户输入、调用BERT模型分析并保存结果。\n",
        "    \"\"\"\n",
        "\n",
        "    # 初始化模型\n",
        "    initialize_classifier()\n",
        "    if not CLASSIFIER:\n",
        "        return\n",
        "\n",
        "    print(f\"\\n--- 1. 正在尝试加载文件: {file_path} ---\")\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        print(f\"成功加载 {len(df)} 篇文献数据。\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"错误: 找不到文件 '{file_path}'。请确保文件存在于当前目录下。\")\n",
        "        return\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"错误: 文件 '{file_path}' 不是一个有效的 JSON 格式。\")\n",
        "        return\n",
        "\n",
        "    # 获取用户输入\n",
        "    disease_name = input(\"\\n--- 2. 请输入您想查找的疾病名称 (例如: 原发性血小板减少症): \")\n",
        "    if not disease_name:\n",
        "        print(\"未输入疾病名称，分析终止。\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n--- 3. 正在对 {len(df)} 篇文献进行 BERT 零样本分析... ---\")\n",
        "\n",
        "    relevance_results: List[Dict[str, Any]] = []\n",
        "\n",
        "    # 使用 tqdm 显示进度条\n",
        "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"分析进度\"):\n",
        "\n",
        "        paper_title = row['title']\n",
        "        paper_abstract = row['abstract']\n",
        "\n",
        "        # 调用 BERT 分类函数\n",
        "        result = classify_relevance_with_bert(\n",
        "            paper_title=paper_title,\n",
        "            paper_abstract=paper_abstract,\n",
        "            disease_name=disease_name\n",
        "        )\n",
        "\n",
        "        # 将分析结果添加到原始数据行中\n",
        "        result_row = row.to_dict()\n",
        "        result_row[f'{disease_name}_相关性'] = result['relevance']\n",
        "        result_row[f'{disease_name}_评估理由'] = result['reason']\n",
        "        relevance_results.append(result_row)\n",
        "\n",
        "    # 结果整合\n",
        "    df_results = pd.DataFrame(relevance_results)\n",
        "\n",
        "    # 结果排序 (可选)\n",
        "    relevance_order = {'高': 3, '中': 2, '低': 1, 'N/A': 0}\n",
        "    sort_column = f'{disease_name}_相关性'\n",
        "    if sort_column in df_results.columns:\n",
        "        df_results['sort_key'] = df_results[sort_column].apply(lambda x: relevance_order.get(x, 0))\n",
        "        df_results = df_results.sort_values(by='sort_key', ascending=False).drop(columns='sort_key')\n",
        "\n",
        "\n",
        "    # --- 4. 结果展示与保存 ---\n",
        "\n",
        "    print(\"\\n--- 4. 分析结果展示 (前5条) ---\")\n",
        "\n",
        "    display_cols = ['id', 'title', 'abstract', f'{disease_name}_相关性', f'{disease_name}_评估理由']\n",
        "    actual_display_cols = [col for col in display_cols if col in df_results.columns]\n",
        "\n",
        "    print(df_results[actual_display_cols].head())\n",
        "\n",
        "    # 保存为 CSV\n",
        "    df_results.to_csv(OUTPUT_FILE_NAME, index=False, encoding='utf-8-sig')\n",
        "    print(f\"\\n--- 5. 结果已保存至: {OUTPUT_FILE_NAME} ---\")\n",
        "    print(\"分析完成。\")\n",
        "\n",
        "# %%\n",
        "# 执行主函数\n",
        "if __name__ == '__main__':\n",
        "    analyze_papers_bert(FILE_PATH)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Z9Pcl3D62-Sq"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}