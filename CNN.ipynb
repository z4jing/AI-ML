{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tizfk0ON9sgV"
      },
      "source": [
        "## Libraries\n",
        "\n",
        "Do not import any additional libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lzNExBwN9sgX"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nepySEK9sgX"
      },
      "source": [
        "## Download dataset and create a data loader\n",
        "PyTorch has two [primitives to work with data](https://pytorch.org/docs/stable/data.html):\n",
        "``torch.utils.data.DataLoader`` and ``torch.utils.data.Dataset``.\n",
        "``Dataset`` stores the samples and their corresponding labels, and ``DataLoader`` wraps an iterable around\n",
        "the ``Dataset``.\n",
        "\n",
        "PyTorch offers domain-specific libraries such as [TorchText](https://pytorch.org/text/stable/index.html),\n",
        "[TorchVision](https://pytorch.org/vision/stable/index.html), and [TorchAudio](https://pytorch.org/audio/stable/index.html),\n",
        "all of which include datasets. For this assignment, we  will be using a TorchVision dataset.\n",
        "\n",
        "The ``torchvision.datasets`` module contains ``Dataset`` objects for many real-world vision data like\n",
        "CIFAR, COCO, MNIST ([full list here](https://pytorch.org/vision/stable/datasets.html)). In this tutorial, we\n",
        "use the CIFAR10 dataset. Every TorchVision ``Dataset`` includes two arguments: ``transform`` and\n",
        "``target_transform`` to modify the samples and labels respectively.\n",
        "\n",
        "We pass the ``Dataset`` as an argument to ``DataLoader``. This wraps an iterable over our dataset, and supports\n",
        "automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element\n",
        "in the dataloader iterable will return a batch of 64 features and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIYLa4De9sgY"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "#download training set\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_dataloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "#download test set\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_dataloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xb4nxsj09sgb"
      },
      "source": [
        "## Creating Models\n",
        "To define a neural network in PyTorch, we create a class that inherits\n",
        "from [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). We define the layers of the network\n",
        "in the ``__init__`` function and specify how data will pass through the network in the ``forward`` function. To accelerate\n",
        "operations in the neural network, we move it to the GPU if available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpBSXtBL9sgb",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 5, padding='same')\n",
        "        self.conv2 = nn.Conv2d(16, 32, 5, padding='same')\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(32*8*8,512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.log_softmax(self.fc2(x), 1)\n",
        "        return x\n",
        "\n",
        "model = CNN().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfM14VqI9sgc"
      },
      "source": [
        "## Optimizing the Model Parameters\n",
        "To train a model, we need a [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "and an [optimizer](https://pytorch.org/docs/stable/optim.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6dwIz7Te9sgc"
      },
      "outputs": [],
      "source": [
        "#negative log likelihood loss\n",
        "loss_fn = nn.NLLLoss()\n",
        "\n",
        "#Adam optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvmlIxNV9sgc"
      },
      "source": [
        "## Training function\n",
        "\n",
        "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and\n",
        "backpropagates the prediction error to adjust the model's parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HnuRU9-K9sgd"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.train()\n",
        "    train_loss, correct = 0, 0\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        train_loss += loss.item()\n",
        "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    average_train_loss = train_loss / num_batches\n",
        "    accuracy = correct / size\n",
        "    return accuracy, average_train_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqWkWSVy9sgd"
      },
      "source": [
        "## Testing function\n",
        "\n",
        "We also check the model's performance against the test dataset to ensure it is learning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AbnkvnVU9sgd"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    average_test_loss = test_loss / num_batches\n",
        "    accuracy = correct / size\n",
        "    return accuracy, average_test_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Avpcl-Wi9sgd"
      },
      "source": [
        "## Training process\n",
        "\n",
        "The training process is conducted over several iterations (*epochs*). During each epoch, the model learns\n",
        "parameters to make better predictions. We print the model's accuracy and loss at each epoch; we'd like to see the\n",
        "accuracy increase and the loss decrease with every epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMx1yZgj9sgd"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "all_train_accuracies = []\n",
        "all_test_accuracies = []\n",
        "for t in tqdm(range(epochs)):\n",
        "    \n",
        "    # train\n",
        "    train_accuracy, average_train_loss = train(train_dataloader, model, loss_fn, optimizer)\n",
        "    all_train_accuracies += [train_accuracy]\n",
        "    \n",
        "    #test\n",
        "    test_accuracy, average_test_loss = test(test_dataloader, model, loss_fn)\n",
        "    all_test_accuracies += [test_accuracy]\n",
        "    \n",
        "    print(f\"Epoch {t+1}:\\t Train accuracy: {100*train_accuracy:0.1f}%\\t Avg train loss: {average_train_loss:>6f}\\t Test accuracy: {100*test_accuracy:0.1f}%\\t Avg test loss: {average_test_loss:>6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX3lpjAm8HsP"
      },
      "source": [
        "## Plot Results\n",
        "\n",
        "Display the training and testing accuracy as a function of the number of epochs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPizhJp58HsP"
      },
      "outputs": [],
      "source": [
        "plt.plot(all_train_accuracies)\n",
        "plt.title('training accuracy')\n",
        "plt.ylabel('training accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Conv'], loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(all_test_accuracies)\n",
        "plt.title('testing accuracy')\n",
        "plt.ylabel('testing accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Conv'], loc='lower right')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
